{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ba812ef",
   "metadata": {},
   "source": [
    "**Week 4: Tuning, Training Controls and Explainability**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2550015b",
   "metadata": {},
   "source": [
    "## Quick start — run these cells in order\n",
    "\n",
    "Follow this minimal sequence to get the notebook running. Use the numbered cell values below (these are the notebook cell numbers starting from 1).\n",
    "\n",
    "1. Cell 2 — Imports & device (run to set up libraries and `device`).\n",
    "2. Cell 4 — Load saved tensors & create DataLoaders (loads `*.pt` files and builds `train_loader`, `val_loader`, `test_loader`).\n",
    "   - If you do not have matching `.pt` files or see a FEATURE_NAMES mismatch, run Cell 21 (see below) to rebuild tensors from the CSV, then re-run Cell 4.\n",
    "3. Cell 6 — Class-balance inspection & `get_pos_weight_tensor` helper (optional; useful when using `pos_weight`).\n",
    "4. Cell 8 — Hyperparameters (`TrainConfig`) — edit training hyperparameters here.\n",
    "5. Cell 10 — Model factory (`TabularFFNN`).\n",
    "6. Cells 14, 16, 15 — Training helpers and entry points:\n",
    "   - Cell 14: `binary_accuracy_from_logits`\n",
    "   - Cell 16: `run_epoch`\n",
    "   - Cell 15: `train_one_run`\n",
    "   (Run these three so the training functions are available.)\n",
    "7. Cell 18 — Infer `input_dim` from `train_loader` and run training (`train_one_run`).\n",
    "\n",
    "Explainability (after restoring or training a model):\n",
    "8. Cell 20 — Resolve `FEATURE_NAMES` (reads CSV header when available).\n",
    "9. Cell 21 — (Optional) Rebuild tensors from CSV — run this if your `.pt` files are missing or misaligned. If you run it, re-run Cell 4 afterwards.\n",
    "10. Cell 23 — Load best checkpoint & rebuild model (`model_explain`).\n",
    "11. Cell 25 — IG config & helpers (builds baseline and IG functions).\n",
    "12. Cell 27 — Run IG global & local visualizations (produces IG plots).\n",
    "13. Cell 29 — SHAP background/sample selection.\n",
    "14. Cell 31 — Run SHAP (DeepExplainer with Kernel fallback) + visuals.\n",
    "\n",
    "Troubleshooting notes:\n",
    "- If you see an IndexError when plotting attributions, it usually means `len(FEATURE_NAMES) != num_features`; run Cell 21 (rebuild tensors) then Cell 4 to fix alignment.\n",
    "- To retrain from scratch, edit hyperparameters in Cell 8 and run Cell 18.\n",
    "\n",
    "That's it — run cells in the order above to reproduce the notebook workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a03fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        learning_rate=0.001,\n",
    "        weight_decay=1e-5,\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.device = device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b17775c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FEATURE_NAMES from feature_order.json (len=21)\n",
      "Loaded SCALER_PARAMS from scaler_params.json (len=0)\n",
      "Canonical feature/scale loader complete.\n"
     ]
    }
   ],
   "source": [
    "# Load canonical feature order and scaler params (if present)\n",
    "import os, json\n",
    "\n",
    "FEATURE_NAMES = globals().get(\"FEATURE_NAMES\", None)\n",
    "SCALER_PARAMS = globals().get(\"SCALER_PARAMS\", {})\n",
    "fo_cands = [\n",
    "    \"feature_order.json\",\n",
    "    \"artifacts_aligned/feature_order.json\",\n",
    "    \"./feature_order.json\",\n",
    "    \"../feature_order.json\",\n",
    "    \"advanced/submissions/team-members/rajan-hans/feature_order.json\",\n",
    "]\n",
    "for p in fo_cands:\n",
    "    if os.path.exists(p):\n",
    "        try:\n",
    "            with open(p, \"r\") as f:\n",
    "                fo = json.load(f)\n",
    "            if isinstance(fo, list) and len(fo) > 0:\n",
    "                FEATURE_NAMES = fo\n",
    "                print(f\"Loaded FEATURE_NAMES from {p} (len={len(FEATURE_NAMES)})\")\n",
    "                break\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "sp_cands = [\n",
    "    \"scaler_params.json\",\n",
    "    \"artifacts_aligned/scaler_params.json\",\n",
    "    \"./scaler_params.json\",\n",
    "    \"../scaler_params.json\",\n",
    "    \"advanced/submissions/team-members/rajan-hans/scaler_params.json\",\n",
    "]\n",
    "for p in sp_cands:\n",
    "    if os.path.exists(p):\n",
    "        try:\n",
    "            with open(p, \"r\") as f:\n",
    "                sc = json.load(f)\n",
    "            if isinstance(sc, dict):\n",
    "                SCALER_PARAMS = sc\n",
    "                print(f\"Loaded SCALER_PARAMS from {p} (len={len(SCALER_PARAMS)})\")\n",
    "                break\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "globals()[\"FEATURE_NAMES\"] = FEATURE_NAMES\n",
    "globals()[\"SCALER_PARAMS\"] = SCALER_PARAMS\n",
    "print(\"Canonical feature/scale loader complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbfeadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟢 Using device: cpu\n",
      "FEATURE_NAMES derived from CSV (len= 21 )\n",
      "Found existing .pt tensors and they match FEATURE_NAMES.\n",
      "🔢 num_features = 21\n",
      "Startup data check complete. If you want to force a rebuild, delete existing .pt files and re-run this cell.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 — Imports, device, and automatic data-check/rebuild\n",
    "# Run this cell first to set up imports, `device`, and to ensure the dataset tensors exist and match the CSV header.\n",
    "# This notebook expects the following files saved in the current working dir (will be auto-created if missing):\n",
    "#   X_train_tensor.pt, y_train_tensor.pt, X_val_tensor.pt, y_val_tensor.pt, X_test_tensor.pt, y_test_tensor.pt\n",
    "\n",
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Optional, Dict, Any\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🟢 Using device: {device}\")\n",
    "\n",
    "# --- Resolve FEATURE_NAMES from CSV (preferred) or fallback list ---\n",
    "csv_candidates = [\n",
    "    \"data/diabetes_binary_health_indicators_BRFSS2015.csv\",\n",
    "    \"./data/diabetes_binary_health_indicators_BRFSS2015.csv\",\n",
    "    \"../data/diabetes_binary_health_indicators_BRFSS2015.csv\",\n",
    "    \"diabetes_binary_health_indicators_BRFSS2015.csv\",\n",
    "]\n",
    "csv_path = next((p for p in csv_candidates if os.path.exists(p)), None)\n",
    "if csv_path is not None:\n",
    "    try:\n",
    "        df_head = pd.read_csv(csv_path, nrows=0)\n",
    "        FEATURE_NAMES = [c for c in df_head.columns if c != \"Diabetes_binary\"]\n",
    "        print(\"FEATURE_NAMES derived from CSV (len=\", len(FEATURE_NAMES), \")\")\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            \"Failed reading CSV header; using fallback FEATURE_NAMES. Error:\", repr(e)\n",
    "        )\n",
    "        csv_path = None\n",
    "\n",
    "if csv_path is None:\n",
    "    FEATURE_NAMES = [\n",
    "        \"HighBP\",\n",
    "        \"HighChol\",\n",
    "        \"CholCheck\",\n",
    "        \"BMI\",\n",
    "        \"Smoker\",\n",
    "        \"Stroke\",\n",
    "        \"HeartDiseaseorAttack\",\n",
    "        \"PhysActivity\",\n",
    "        \"Fruits\",\n",
    "        \"Veggies\",\n",
    "        \"HvyAlcoholConsump\",\n",
    "        \"AnyHealthcare\",\n",
    "        \"NoDocbcCost\",\n",
    "        \"GenHlth\",\n",
    "        \"MentHlth\",\n",
    "        \"PhysHlth\",\n",
    "        \"DiffWalk\",\n",
    "        \"Sex\",\n",
    "        \"Age\",\n",
    "        \"Education\",\n",
    "        \"Income\",\n",
    "    ]\n",
    "    print(\"Using fallback FEATURE_NAMES (len=\", len(FEATURE_NAMES), \")\")\n",
    "\n",
    "# --- Ensure .pt tensors exist and match FEATURE_NAMES; rebuild from CSV if needed ---\n",
    "pt_files = [\n",
    "    \"X_train_tensor.pt\",\n",
    "    \"y_train_tensor.pt\",\n",
    "    \"X_val_tensor.pt\",\n",
    "    \"y_val_tensor.pt\",\n",
    "    \"X_test_tensor.pt\",\n",
    "    \"y_test_tensor.pt\",\n",
    "]\n",
    "\n",
    "need_rebuild = False\n",
    "# If any file is missing, mark for rebuild\n",
    "if not all(os.path.exists(p) for p in pt_files):\n",
    "    print(\"One or more .pt files missing; will rebuild from CSV if available.\")\n",
    "    need_rebuild = True\n",
    "else:\n",
    "    # If all present, verify the feature count matches\n",
    "    try:\n",
    "        X_train_tmp = torch.load(\"X_train_tensor.pt\")\n",
    "        if X_train_tmp.ndim != 2 or X_train_tmp.shape[1] != len(FEATURE_NAMES):\n",
    "            print(\n",
    "                \"Existing tensors have feature-count mismatch:\",\n",
    "                getattr(X_train_tmp, \"shape\", None),\n",
    "                \"vs FEATURE_NAMES len=\",\n",
    "                len(FEATURE_NAMES),\n",
    "            )\n",
    "            need_rebuild = True\n",
    "        else:\n",
    "            print(\"Found existing .pt tensors and they match FEATURE_NAMES.\")\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            \"Error loading existing .pt files; will rebuild if possible. Error:\",\n",
    "            repr(e),\n",
    "        )\n",
    "        need_rebuild = True\n",
    "\n",
    "if need_rebuild:\n",
    "    if csv_path is None:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Cannot rebuild: CSV not found in candidates {csv_candidates} and .pt files are missing or invalid.\"\n",
    "        )\n",
    "\n",
    "    print(\"Rebuilding tensors from CSV:\", csv_path)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    target_col = \"Diabetes_binary\"\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"Expected target column '{target_col}' in CSV columns\")\n",
    "\n",
    "    # Derive FEATURE_NAMES from CSV to be sure\n",
    "    FEATURE_NAMES = [c for c in df.columns if c != target_col]\n",
    "\n",
    "    # Minimal preprocessing: fill numeric NA with column mean\n",
    "    df_work = df[FEATURE_NAMES + [target_col]].copy()\n",
    "    for c in FEATURE_NAMES:\n",
    "        if df_work[c].isna().any():\n",
    "            df_work[c].fillna(df_work[c].mean(), inplace=True)\n",
    "\n",
    "    X = df_work[FEATURE_NAMES].to_numpy(dtype=np.float32)\n",
    "    y = df_work[target_col].to_numpy(dtype=np.float32)\n",
    "    print(\"Full dataset shape X, y:\", X.shape, y.shape)\n",
    "\n",
    "    # Stratified split: test 15%, val 15%, train rest\n",
    "    test_frac = 0.15\n",
    "    val_frac = 0.15\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=test_frac, stratify=y, random_state=42\n",
    "    )\n",
    "    val_rel = val_frac / (1.0 - test_frac)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_rel, stratify=y_temp, random_state=42\n",
    "    )\n",
    "    print(\"Split shapes:\")\n",
    "    print(\"  X_train, y_train:\", X_train.shape, y_train.shape)\n",
    "    print(\"  X_val,   y_val:  \", X_val.shape, y_val.shape)\n",
    "    print(\"  X_test,  y_test: \", X_test.shape, y_test.shape)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    X_train_tensor = torch.from_numpy(X_train)\n",
    "    X_val_tensor = torch.from_numpy(X_val)\n",
    "    X_test_tensor = torch.from_numpy(X_test)\n",
    "\n",
    "    y_train_tensor = torch.from_numpy(y_train).float()\n",
    "    y_val_tensor = torch.from_numpy(y_val).float()\n",
    "    y_test_tensor = torch.from_numpy(y_test).float()\n",
    "\n",
    "    # Backup existing files then save new tensors\n",
    "    for fname, var in [\n",
    "        (\"X_train_tensor.pt\", X_train_tensor),\n",
    "        (\"y_train_tensor.pt\", y_train_tensor),\n",
    "        (\"X_val_tensor.pt\", X_val_tensor),\n",
    "        (\"y_val_tensor.pt\", y_val_tensor),\n",
    "        (\"X_test_tensor.pt\", X_test_tensor),\n",
    "        (\"y_test_tensor.pt\", y_test_tensor),\n",
    "    ]:\n",
    "        if os.path.exists(fname):\n",
    "            bak = f\"{fname}.bak.{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "            os.rename(fname, bak)\n",
    "            print(f\"Backed up {fname} -> {bak}\")\n",
    "        torch.save(var, fname)\n",
    "        print(f\"Saved {fname} (shape={tuple(var.shape)})\")\n",
    "\n",
    "    num_features = X_train_tensor.shape[1]\n",
    "    print(f\"🔢 Updated num_features = {num_features}\")\n",
    "    print(\"Rebuild complete.\")\n",
    "else:\n",
    "    # Load tensors into workspace variables so subsequent cells can use them\n",
    "    X_train_tensor = torch.load(\"X_train_tensor.pt\")\n",
    "    y_train_tensor = torch.load(\"y_train_tensor.pt\")\n",
    "    X_val_tensor = torch.load(\"X_val_tensor.pt\")\n",
    "    y_val_tensor = torch.load(\"y_val_tensor.pt\")\n",
    "    X_test_tensor = torch.load(\"X_test_tensor.pt\")\n",
    "    y_test_tensor = torch.load(\"y_test_tensor.pt\")\n",
    "    num_features = X_train_tensor.shape[1]\n",
    "    print(f\"🔢 num_features = {num_features}\")\n",
    "\n",
    "print(\n",
    "    \"Startup data check complete. If you want to force a rebuild, delete existing .pt files and re-run this cell.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee6e746",
   "metadata": {},
   "source": [
    "Rebuild DataLoaders from saved tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5b468a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 num_features = 21\n",
      "✅ Data loaded and DataLoaders created.\n"
     ]
    }
   ],
   "source": [
    "X_train_tensor = torch.load(\"X_train_tensor.pt\")\n",
    "y_train_tensor = torch.load(\"y_train_tensor.pt\")\n",
    "X_val_tensor = torch.load(\"X_val_tensor.pt\")\n",
    "y_val_tensor = torch.load(\"y_val_tensor.pt\")\n",
    "X_test_tensor = torch.load(\"X_test_tensor.pt\")\n",
    "y_test_tensor = torch.load(\"y_test_tensor.pt\")\n",
    "\n",
    "\n",
    "# Quick sanity checks\n",
    "assert X_train_tensor.ndim == 2, \"X_train must be [N, num_features]\"\n",
    "assert (\n",
    "    X_val_tensor.shape[1] == X_train_tensor.shape[1]\n",
    "), \"val features != train features\"\n",
    "assert (\n",
    "    X_test_tensor.shape[1] == X_train_tensor.shape[1]\n",
    "), \"test features != train features\"\n",
    "assert y_train_tensor.shape[0] == X_train_tensor.shape[0], \"y_train length mismatch\"\n",
    "assert y_val_tensor.shape[0] == X_val_tensor.shape[0], \"y_val length mismatch\"\n",
    "assert y_test_tensor.shape[0] == X_test_tensor.shape[0], \"y_test length mismatch\"\n",
    "\n",
    "num_features = X_train_tensor.shape[1]\n",
    "print(f\"🔢 num_features = {num_features}\")\n",
    "\n",
    "\n",
    "# Build datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Loader knobs (easy to tweak)\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = min(4, os.cpu_count() or 0)\n",
    "PIN_MEMORY = device.type == \"cuda\"\n",
    "DROP_LAST_TRAIN = True  # helps when using BatchNorm\n",
    "\n",
    "# Build DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    drop_last=DROP_LAST_TRAIN,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    drop_last=False,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "print(\"✅ Data loaded and DataLoaders created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d9e2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FEATURE_NAMES matches num_features (21)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — Quick verification: FEATURE_NAMES vs num_features\n",
    "# Raises an assertion error early if the CSV-derived feature names and tensors are misaligned.\n",
    "assert (\n",
    "    len(FEATURE_NAMES) == num_features\n",
    "), f\"FEATURE_NAMES len={len(FEATURE_NAMES)} vs num_features={num_features}\"\n",
    "print(f\"✅ FEATURE_NAMES matches num_features ({num_features})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aea4c27",
   "metadata": {},
   "source": [
    "Class balance (optional) & pos_weight helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3c18c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Train positives: 24742 (0.139), negatives: 152834 (0.861), total: 177576\n"
     ]
    }
   ],
   "source": [
    "# pos_weight for BCEWithLogitsLoss\n",
    "with torch.no_grad():\n",
    "    pos_count = (y_train_tensor == 1).sum().item()\n",
    "    neg_count = (y_train_tensor == 0).sum().item()\n",
    "    total = len(y_train_tensor)\n",
    "    pos_ratio = pos_count / total\n",
    "    print(\n",
    "        f\"📊 Train positives: {pos_count} ({pos_ratio:.3f}), negatives: {neg_count} ({neg_count/total:.3f}), total: {total}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_pos_weight_tensor(y_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    # pos_weight = N_neg / N_pos (used by BCEWithLogitsLoss)\n",
    "    pos = (y_tensor == 1).sum().item()\n",
    "    neg = (y_tensor == 0).sum().item()\n",
    "    assert pos > 0 and neg > 0, \"Need at least one positive and one negative\"\n",
    "    return torch.tensor([neg / pos], dtype=torch.float32, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0a87c",
   "metadata": {},
   "source": [
    "Hyperparameters (single place to tweak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92445ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainConfig(hidden_sizes=[256, 128, 64], dropout=0.35, use_batchnorm=True, activation='relu', optimizer='adamw', lr=0.001, weight_decay=0.0001, momentum=0.9, epochs=60, grad_clip=1.0, mixed_precision=True, early_stopping=True, patience=10, min_delta=0.0001, scheduler='plateau', step_size=12, gamma=0.5, cosine_T_max=30, plateau_factor=0.5, plateau_patience=3, plateau_min_lr=1e-06, use_pos_weight=False, save_dir='checkpoints', run_name='run_20250908_215357', print_every=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "import time\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    # Architecture\n",
    "    hidden_sizes: List[int] = None  # e.g. [256, 128, 64]\n",
    "    dropout: float = 0.35\n",
    "    use_batchnorm: bool = True\n",
    "    activation: str = \"relu\"  # \"relu\" | \"gelu\" | \"leaky_relu\"\n",
    "\n",
    "    # Optimization\n",
    "    optimizer: str = \"adamw\"\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    momentum: float = 0.9\n",
    "\n",
    "    # Training\n",
    "    epochs: int = 60\n",
    "    grad_clip: Optional[float] = 1.0\n",
    "    mixed_precision: bool = True\n",
    "\n",
    "    # Early Stopping\n",
    "    early_stopping: bool = True\n",
    "    patience: int = 10\n",
    "    min_delta: float = 1e-4\n",
    "\n",
    "    # LR Scheduler\n",
    "    scheduler: str = \"plateau\"\n",
    "    step_size: int = 12\n",
    "    gamma: float = 0.5\n",
    "    cosine_T_max: int = 30\n",
    "    plateau_factor: float = 0.5\n",
    "    plateau_patience: int = 3\n",
    "    plateau_min_lr: float = 1e-6\n",
    "\n",
    "    # Loss options\n",
    "    use_pos_weight: bool = False\n",
    "\n",
    "    # Saving\n",
    "    save_dir: str = \"checkpoints\"\n",
    "    run_name: Optional[str] = None\n",
    "\n",
    "    # Logging\n",
    "    print_every: int = 1\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.hidden_sizes is None:\n",
    "            self.hidden_sizes = [256, 128, 64]\n",
    "        if self.run_name is None:\n",
    "            self.run_name = time.strftime(\"run_%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "# create instance\n",
    "cfg = TrainConfig()\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dafbc2",
   "metadata": {},
   "source": [
    "Model factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc53c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularFFNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_sizes: List[int],\n",
    "        output_dim: int = 1,\n",
    "        dropout: float = 0.3,\n",
    "        use_batchnorm: bool = True,\n",
    "        activation: str = \"relu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        acts = {\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"gelu\": nn.GELU(),\n",
    "            \"leaky_relu\": nn.LeakyReLU(0.01),\n",
    "        }\n",
    "        assert activation in acts, f\"Unsupported activation: {activation}\"\n",
    "        self.activation = acts[activation]\n",
    "\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(self.activation)\n",
    "            if dropout and dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, 1))  # 1 logit for binary classification\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)  # [B]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce22c177",
   "metadata": {},
   "source": [
    "Optimizers, schedulers, EarlyStopping, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde3ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_scheduler(optimizer, cfg: TrainConfig):\n",
    "    if cfg.scheduler == \"none\":\n",
    "        return None\n",
    "    if cfg.scheduler == \"step\":\n",
    "        return torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer, step_size=cfg.step_size, gamma=cfg.gamma\n",
    "        )\n",
    "    if cfg.scheduler == \"cosine\":\n",
    "        return torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=cfg.cosine_T_max\n",
    "        )\n",
    "    if cfg.scheduler == \"plateau\":\n",
    "        return torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=cfg.plateau_factor,\n",
    "            patience=cfg.plateau_patience,\n",
    "            min_lr=cfg.plateau_min_lr,\n",
    "        )\n",
    "    raise ValueError(f\"Unsupported scheduler: {cfg.scheduler}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5833defa",
   "metadata": {},
   "source": [
    "Train/validate loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f64bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy_from_logits(logits, targets, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes binary accuracy given logits and targets (float or int, shape [B]).\n",
    "    \"\"\"\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs >= threshold).float()\n",
    "    targets = targets.float()\n",
    "    correct = (preds == targets).float().sum()\n",
    "    return correct.item() / len(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ded2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_run(cfg, train_loader, val_loader, input_dim):\n",
    "    \"\"\"\n",
    "    Trains a TabularFFNN model using the provided config and loaders.\n",
    "    Returns a dict with best val loss and checkpoint path.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import copy\n",
    "    from torch.cuda.amp import GradScaler\n",
    "\n",
    "    # Model\n",
    "    model = TabularFFNN(\n",
    "        input_dim=input_dim,\n",
    "        hidden_sizes=cfg.hidden_sizes,\n",
    "        dropout=cfg.dropout,\n",
    "        use_batchnorm=cfg.use_batchnorm,\n",
    "        activation=cfg.activation,\n",
    "    ).to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    if cfg.optimizer == \"adamw\":\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay\n",
    "        )\n",
    "    elif cfg.optimizer == \"adam\":\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay\n",
    "        )\n",
    "    elif cfg.optimizer == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=cfg.lr,\n",
    "            weight_decay=cfg.weight_decay,\n",
    "            momentum=cfg.momentum,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {cfg.optimizer}\")\n",
    "\n",
    "    # Loss\n",
    "    if cfg.use_pos_weight:\n",
    "        pos_weight = get_pos_weight_tensor(y_train_tensor)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Scheduler\n",
    "    scheduler = make_scheduler(optimizer, cfg)\n",
    "\n",
    "    # AMP scaler\n",
    "    scaler = GradScaler() if cfg.mixed_precision and device.type == \"cuda\" else None\n",
    "\n",
    "    # Early stopping\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_state = None\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    save_dir = os.path.join(cfg.save_dir, cfg.run_name)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    best_path = os.path.join(save_dir, \"best_model.pt\")\n",
    "\n",
    "    for epoch in range(cfg.epochs):\n",
    "        train_loss, train_acc = run_epoch(\n",
    "            model, train_loader, criterion, optimizer, cfg, scaler\n",
    "        )\n",
    "        val_loss, val_acc = run_epoch(model, val_loader, criterion, None, cfg)\n",
    "\n",
    "        # Scheduler step\n",
    "        if scheduler:\n",
    "            if cfg.scheduler == \"plateau\":\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss - cfg.min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = {\n",
    "                \"model_state\": copy.deepcopy(model.state_dict()),\n",
    "                \"cfg\": asdict(cfg),\n",
    "                \"input_dim\": input_dim,\n",
    "                \"hidden_sizes\": cfg.hidden_sizes,\n",
    "            }\n",
    "            torch.save(best_state, best_path)\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if cfg.print_every and (\n",
    "            epoch % cfg.print_every == 0 or epoch == cfg.epochs - 1\n",
    "        ):\n",
    "            print(\n",
    "                f\"Epoch {epoch+1:3d}/{cfg.epochs} | Train loss: {train_loss:.4f} | Val loss: {val_loss:.4f} | Train acc: {train_acc:.3f} | Val acc: {val_acc:.3f}\"\n",
    "            )\n",
    "\n",
    "        if cfg.early_stopping and patience_counter >= cfg.patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Load best state for return\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state[\"model_state\"])\n",
    "    else:\n",
    "        print(\"Warning: No improvement during training.\")\n",
    "\n",
    "    return {\"best_val_loss\": best_val_loss, \"best_path\": best_path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a54f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(\n",
    "    model, loader, criterion, optimizer=None, cfg: TrainConfig = None, scaler=None\n",
    "):\n",
    "    is_train = optimizer is not None\n",
    "    model.train(is_train)\n",
    "\n",
    "    running_loss, running_acc, n_batches = 0.0, 0.0, 0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True).float()\n",
    "        yb = yb.to(\n",
    "            device, non_blocking=True\n",
    "        ).float()  # BCEWithLogits expects float targets\n",
    "        yb = yb.view(-1)  # Flatten target to shape [B]\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            use_amp = cfg.mixed_precision and device.type == \"cuda\"\n",
    "            amp_ctx = (\n",
    "                torch.autocast(device_type=\"cuda\", dtype=torch.float16)\n",
    "                if use_amp\n",
    "                else nullcontext()\n",
    "            )\n",
    "\n",
    "            with amp_ctx:\n",
    "                logits = model(xb)  # [B]\n",
    "                loss = criterion(logits, yb)  # scalar\n",
    "\n",
    "            if scaler is not None and use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                if cfg.grad_clip:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                if cfg.grad_clip:\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "                optimizer.step()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "\n",
    "        acc = binary_accuracy_from_logits(logits, yb)\n",
    "        running_loss += loss.item()\n",
    "        running_acc += acc\n",
    "        n_batches += 1\n",
    "\n",
    "    return running_loss / n_batches, running_acc / n_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a096e",
   "metadata": {},
   "source": [
    "Infer input_dim, run training (edit hyperparams above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57849e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Inferred input_dim = 21\n",
      "Epoch   1/60 | Train loss: 0.3327 | Val loss: 0.3152 | Train acc: 0.859 | Val acc: 0.864\n",
      "Epoch   1/60 | Train loss: 0.3327 | Val loss: 0.3152 | Train acc: 0.859 | Val acc: 0.864\n",
      "Epoch   2/60 | Train loss: 0.3212 | Val loss: 0.3120 | Train acc: 0.863 | Val acc: 0.867\n",
      "Epoch   2/60 | Train loss: 0.3212 | Val loss: 0.3120 | Train acc: 0.863 | Val acc: 0.867\n",
      "Epoch   3/60 | Train loss: 0.3199 | Val loss: 0.3135 | Train acc: 0.863 | Val acc: 0.865\n",
      "Epoch   3/60 | Train loss: 0.3199 | Val loss: 0.3135 | Train acc: 0.863 | Val acc: 0.865\n",
      "Epoch   4/60 | Train loss: 0.3192 | Val loss: 0.3128 | Train acc: 0.863 | Val acc: 0.866\n",
      "Epoch   4/60 | Train loss: 0.3192 | Val loss: 0.3128 | Train acc: 0.863 | Val acc: 0.866\n",
      "Epoch   5/60 | Train loss: 0.3186 | Val loss: 0.3117 | Train acc: 0.864 | Val acc: 0.867\n",
      "Epoch   5/60 | Train loss: 0.3186 | Val loss: 0.3117 | Train acc: 0.864 | Val acc: 0.867\n",
      "Epoch   6/60 | Train loss: 0.3176 | Val loss: 0.3119 | Train acc: 0.864 | Val acc: 0.866\n",
      "Epoch   6/60 | Train loss: 0.3176 | Val loss: 0.3119 | Train acc: 0.864 | Val acc: 0.866\n",
      "Epoch   7/60 | Train loss: 0.3178 | Val loss: 0.3133 | Train acc: 0.864 | Val acc: 0.866\n",
      "Epoch   7/60 | Train loss: 0.3178 | Val loss: 0.3133 | Train acc: 0.864 | Val acc: 0.866\n",
      "Epoch   8/60 | Train loss: 0.3171 | Val loss: 0.3124 | Train acc: 0.864 | Val acc: 0.865\n",
      "Epoch   8/60 | Train loss: 0.3171 | Val loss: 0.3124 | Train acc: 0.864 | Val acc: 0.865\n",
      "Epoch   9/60 | Train loss: 0.3169 | Val loss: 0.3111 | Train acc: 0.864 | Val acc: 0.866\n",
      "Epoch   9/60 | Train loss: 0.3169 | Val loss: 0.3111 | Train acc: 0.864 | Val acc: 0.866\n",
      "Epoch  10/60 | Train loss: 0.3166 | Val loss: 0.3109 | Train acc: 0.864 | Val acc: 0.867\n",
      "Epoch  10/60 | Train loss: 0.3166 | Val loss: 0.3109 | Train acc: 0.864 | Val acc: 0.867\n",
      "Epoch  11/60 | Train loss: 0.3169 | Val loss: 0.3115 | Train acc: 0.864 | Val acc: 0.865\n",
      "Epoch  11/60 | Train loss: 0.3169 | Val loss: 0.3115 | Train acc: 0.864 | Val acc: 0.865\n",
      "Epoch  12/60 | Train loss: 0.3165 | Val loss: 0.3119 | Train acc: 0.864 | Val acc: 0.866\n",
      "Epoch  12/60 | Train loss: 0.3165 | Val loss: 0.3119 | Train acc: 0.864 | Val acc: 0.866\n",
      "Epoch  13/60 | Train loss: 0.3160 | Val loss: 0.3116 | Train acc: 0.864 | Val acc: 0.866\n",
      "Epoch  13/60 | Train loss: 0.3160 | Val loss: 0.3116 | Train acc: 0.864 | Val acc: 0.866\n",
      "Epoch  14/60 | Train loss: 0.3162 | Val loss: 0.3138 | Train acc: 0.864 | Val acc: 0.866\n",
      "Epoch  14/60 | Train loss: 0.3162 | Val loss: 0.3138 | Train acc: 0.864 | Val acc: 0.866\n",
      "Epoch  15/60 | Train loss: 0.3151 | Val loss: 0.3108 | Train acc: 0.865 | Val acc: 0.867\n",
      "Epoch  15/60 | Train loss: 0.3151 | Val loss: 0.3108 | Train acc: 0.865 | Val acc: 0.867\n",
      "Epoch  16/60 | Train loss: 0.3154 | Val loss: 0.3112 | Train acc: 0.865 | Val acc: 0.866\n",
      "Epoch  16/60 | Train loss: 0.3154 | Val loss: 0.3112 | Train acc: 0.865 | Val acc: 0.866\n",
      "Epoch  17/60 | Train loss: 0.3152 | Val loss: 0.3103 | Train acc: 0.865 | Val acc: 0.867\n",
      "Epoch  17/60 | Train loss: 0.3152 | Val loss: 0.3103 | Train acc: 0.865 | Val acc: 0.867\n",
      "Epoch  18/60 | Train loss: 0.3149 | Val loss: 0.3115 | Train acc: 0.865 | Val acc: 0.867\n",
      "Epoch  18/60 | Train loss: 0.3149 | Val loss: 0.3115 | Train acc: 0.865 | Val acc: 0.867\n",
      "Epoch  19/60 | Train loss: 0.3148 | Val loss: 0.3110 | Train acc: 0.865 | Val acc: 0.867\n",
      "Epoch  19/60 | Train loss: 0.3148 | Val loss: 0.3110 | Train acc: 0.865 | Val acc: 0.867\n",
      "Epoch  20/60 | Train loss: 0.3146 | Val loss: 0.3104 | Train acc: 0.865 | Val acc: 0.867\n",
      "Epoch  20/60 | Train loss: 0.3146 | Val loss: 0.3104 | Train acc: 0.865 | Val acc: 0.867\n",
      "Epoch  21/60 | Train loss: 0.3146 | Val loss: 0.3108 | Train acc: 0.865 | Val acc: 0.866\n",
      "Epoch  21/60 | Train loss: 0.3146 | Val loss: 0.3108 | Train acc: 0.865 | Val acc: 0.866\n",
      "Epoch  22/60 | Train loss: 0.3142 | Val loss: 0.3102 | Train acc: 0.865 | Val acc: 0.868\n",
      "Epoch  22/60 | Train loss: 0.3142 | Val loss: 0.3102 | Train acc: 0.865 | Val acc: 0.868\n",
      "Epoch  23/60 | Train loss: 0.3144 | Val loss: 0.3105 | Train acc: 0.865 | Val acc: 0.867\n",
      "Epoch  23/60 | Train loss: 0.3144 | Val loss: 0.3105 | Train acc: 0.865 | Val acc: 0.867\n",
      "Epoch  24/60 | Train loss: 0.3141 | Val loss: 0.3104 | Train acc: 0.865 | Val acc: 0.867\n",
      "Epoch  24/60 | Train loss: 0.3141 | Val loss: 0.3104 | Train acc: 0.865 | Val acc: 0.867\n",
      "Epoch  25/60 | Train loss: 0.3142 | Val loss: 0.3104 | Train acc: 0.865 | Val acc: 0.868\n",
      "Epoch  25/60 | Train loss: 0.3142 | Val loss: 0.3104 | Train acc: 0.865 | Val acc: 0.868\n",
      "Epoch  26/60 | Train loss: 0.3141 | Val loss: 0.3104 | Train acc: 0.865 | Val acc: 0.867\n",
      "Epoch  26/60 | Train loss: 0.3141 | Val loss: 0.3104 | Train acc: 0.865 | Val acc: 0.867\n",
      "Epoch  27/60 | Train loss: 0.3133 | Val loss: 0.3105 | Train acc: 0.865 | Val acc: 0.867\n",
      "Epoch  27/60 | Train loss: 0.3133 | Val loss: 0.3105 | Train acc: 0.865 | Val acc: 0.867\n",
      "Epoch  28/60 | Train loss: 0.3138 | Val loss: 0.3104 | Train acc: 0.866 | Val acc: 0.867\n",
      "Epoch  28/60 | Train loss: 0.3138 | Val loss: 0.3104 | Train acc: 0.866 | Val acc: 0.867\n",
      "Epoch  29/60 | Train loss: 0.3140 | Val loss: 0.3103 | Train acc: 0.866 | Val acc: 0.867\n",
      "Epoch  29/60 | Train loss: 0.3140 | Val loss: 0.3103 | Train acc: 0.866 | Val acc: 0.867\n",
      "Epoch  30/60 | Train loss: 0.3135 | Val loss: 0.3107 | Train acc: 0.865 | Val acc: 0.867\n",
      "Epoch  30/60 | Train loss: 0.3135 | Val loss: 0.3107 | Train acc: 0.865 | Val acc: 0.867\n",
      "Epoch  31/60 | Train loss: 0.3138 | Val loss: 0.3103 | Train acc: 0.865 | Val acc: 0.867\n",
      "Epoch  31/60 | Train loss: 0.3138 | Val loss: 0.3103 | Train acc: 0.865 | Val acc: 0.867\n",
      "Epoch  32/60 | Train loss: 0.3137 | Val loss: 0.3106 | Train acc: 0.865 | Val acc: 0.867\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 0.31021576576373155\n",
      "Best model path: checkpoints\\run_20250908_215357\\best_model.pt\n",
      "Epoch  32/60 | Train loss: 0.3137 | Val loss: 0.3106 | Train acc: 0.865 | Val acc: 0.867\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 0.31021576576373155\n",
      "Best model path: checkpoints\\run_20250908_215357\\best_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Make sure to run all previous cells that define train_one_run and its dependencies before running this cell.\n",
    "# Infer input_dim from one batch\n",
    "xb0, yb0 = next(iter(train_loader))\n",
    "input_dim = xb0.shape[1]\n",
    "print(f\"🔎 Inferred input_dim = {input_dim}\")\n",
    "\n",
    "# (Optional) align drop_last to batchnorm choice\n",
    "if cfg.use_batchnorm and not DROP_LAST_TRAIN:\n",
    "    print(\n",
    "        \"⚠️ Consider setting DROP_LAST_TRAIN=True when using BatchNorm to avoid tiny last batches.\"\n",
    "    )\n",
    "\n",
    "result = train_one_run(cfg, train_loader, val_loader, input_dim)\n",
    "print(\"Best val loss:\", result[\"best_val_loss\"])\n",
    "print(\"Best model path:\", result[\"best_path\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd04995",
   "metadata": {},
   "source": [
    "Install (if needed) & imports for explainers - Do not run since it is done above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3267adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve FEATURE_NAMES: prefer CSV header if available, otherwise fallback to hardcoded list\n",
    "import os\n",
    "\n",
    "csv_path = \"data/diabetes_binary_health_indicators_BRFSS2015.csv\"\n",
    "if os.path.exists(csv_path):\n",
    "    import pandas as pd\n",
    "\n",
    "    df_head = pd.read_csv(csv_path, nrows=0)\n",
    "    FEATURE_NAMES = [c for c in df_head.columns if c != \"Diabetes_binary\"]\n",
    "else:\n",
    "    # Fallback list (kept for offline editing)\n",
    "    FEATURE_NAMES = [\n",
    "        \"HighBP\",\n",
    "        \"HighChol\",\n",
    "        \"CholCheck\",\n",
    "        \"BMI\",\n",
    "        \"Smoker\",\n",
    "        \"Stroke\",\n",
    "        \"HeartDiseaseorAttack\",\n",
    "        \"PhysActivity\",\n",
    "        \"Fruits\",\n",
    "        \"Veggies\",\n",
    "        \"HvyAlcoholConsump\",\n",
    "        \"AnyHealthcare\",\n",
    "        \"NoDocbcCost\",\n",
    "        \"GenHlth\",\n",
    "        \"MentHlth\",\n",
    "        \"PhysHlth\",\n",
    "        \"DiffWalk\",\n",
    "        \"Sex\",\n",
    "        \"Age\",\n",
    "        \"Education\",\n",
    "        \"Income\",\n",
    "    ]\n",
    "\n",
    "# Integrated Gradients implemented below; FEATURE_NAMES now available from CSV when possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f49aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21 — Rebuild-from-CSV (deprecated shortcut)\n",
    "# The notebook now performs startup data validation and rebuild in the top cell.\n",
    "# This cell is kept for compatibility but does not need to be run in normal workflows.\n",
    "print(\n",
    "    \"Note: startup cell already performs CSV-based rebuild if necessary. Delete .pt files and re-run the top cell to force a rebuild.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba29aa8",
   "metadata": {},
   "source": [
    "Load best checkpoint & rebuild the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911839b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate best checkpoint path\n",
    "if \"result\" in globals() and isinstance(result, dict) and \"best_path\" in result:\n",
    "    BEST_PATH = result[\"best_path\"]\n",
    "    print(\"Using best model from training:\", BEST_PATH)\n",
    "else:\n",
    "    # Fallback: set manually if needed\n",
    "    # BEST_PATH = \"checkpoints/run_YYYYMMDD_HHMMSS/best_model.pt\"\n",
    "    raise FileNotFoundError(\n",
    "        \"No result['best_path'] found. Re-run training cell or set BEST_PATH manually.\"\n",
    "    )\n",
    "\n",
    "ckpt = torch.load(BEST_PATH, map_location=device)\n",
    "cfg_loaded = ckpt.get(\"cfg\", {})\n",
    "hidden_sizes = ckpt.get(\"hidden_sizes\", [256, 128, 64])\n",
    "input_dim_ckpt = ckpt.get(\"input_dim\", X_val_tensor.shape[1])\n",
    "\n",
    "model_explain = TabularFFNN(\n",
    "    input_dim=input_dim_ckpt,\n",
    "    hidden_sizes=hidden_sizes,\n",
    "    dropout=cfg_loaded.get(\"dropout\", 0.0),\n",
    "    use_batchnorm=cfg_loaded.get(\"use_batchnorm\", True),\n",
    "    activation=cfg_loaded.get(\"activation\", \"relu\"),\n",
    ").to(device)\n",
    "model_explain.load_state_dict(ckpt[\"model_state\"])\n",
    "model_explain.eval()\n",
    "print(\"✅ Model restored & set to eval().\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d677851b",
   "metadata": {},
   "source": [
    "IG config & helpers (global + local attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d585f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config for explainers ---\n",
    "class ExplainConfig:\n",
    "    # Integrated Gradients\n",
    "    ig_steps: int = 64  # 32–128 is common\n",
    "    ig_batch_size: int = 512  # batch IG for speed\n",
    "    ig_baseline: str = \"mean\"  # \"zero\" | \"mean\"\n",
    "    ig_global_samples: int = (\n",
    "        2000  # how many val rows to aggregate for global importance (None = all)\n",
    "    )\n",
    "    top_k: int = 20  # top features to plot\n",
    "\n",
    "\n",
    "EXPL_CFG = ExplainConfig()\n",
    "\n",
    "# Build baseline\n",
    "with torch.no_grad():\n",
    "    if EXPL_CFG.ig_baseline == \"zero\":\n",
    "        ig_baseline_vec = torch.zeros(\n",
    "            (1, num_features), dtype=torch.float32, device=device\n",
    "        )\n",
    "    elif EXPL_CFG.ig_baseline == \"mean\":\n",
    "        ig_baseline_vec = X_train_tensor.mean(dim=0, keepdim=True).to(device)\n",
    "    else:\n",
    "        raise ValueError(\"ig_baseline must be 'zero' or 'mean'\")\n",
    "\n",
    "\n",
    "def integrated_gradients(model, inputs, baseline, n_steps=64):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Integrated Gradients for tabular data.\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        inputs: [B, F] tensor\n",
    "        baseline: [1, F] or [B, F] tensor\n",
    "        n_steps: number of steps for IG\n",
    "    Returns:\n",
    "        attributions: [B, F] tensor (on CPU)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    inputs = inputs.detach()\n",
    "    baseline = baseline.detach()\n",
    "    # Expand baseline if needed\n",
    "    if baseline.shape[0] == 1:\n",
    "        baseline = baseline.expand(inputs.shape[0], -1)\n",
    "    # Generate scaled inputs\n",
    "    alphas = torch.linspace(0, 1, n_steps, device=inputs.device).view(\n",
    "        -1, 1, 1\n",
    "    )  # [n_steps, 1, 1]\n",
    "    baseline = baseline.unsqueeze(0)  # [1, B, F]\n",
    "    inputs = inputs.unsqueeze(0)  # [1, B, F]\n",
    "    interpolated = baseline + alphas * (inputs - baseline)  # [n_steps, B, F]\n",
    "    interpolated = interpolated.requires_grad_()\n",
    "    grads = []\n",
    "    for i in range(n_steps):\n",
    "        x = interpolated[i]  # [B, F]\n",
    "        x.requires_grad_()\n",
    "        out = model(x)  # [B]\n",
    "        out = out.sum()\n",
    "        grad = torch.autograd.grad(out, x, retain_graph=True)[0]  # [B, F]\n",
    "        grads.append(grad)\n",
    "    grads = torch.stack(grads, dim=0)  # [n_steps, B, F]\n",
    "    avg_grads = grads.mean(dim=0)  # [B, F]\n",
    "    attributions = (inputs.squeeze(0) - baseline.squeeze(0)) * avg_grads  # [B, F]\n",
    "    return attributions.detach().cpu()\n",
    "\n",
    "\n",
    "def compute_ig_batch(model, xb, baseline, n_steps=64):\n",
    "    \"\"\"\n",
    "    xb: [B, F] on device; baseline: [1, F] or [B, F]\n",
    "    returns attributions [B, F] on CPU\n",
    "    \"\"\"\n",
    "    return integrated_gradients(model, xb, baseline, n_steps)\n",
    "\n",
    "\n",
    "def ig_global_importance(model, X_tensor, cfg: ExplainConfig):\n",
    "    \"\"\"\n",
    "    Aggregates |IG| over a subset of X_tensor to get global importances.\n",
    "    \"\"\"\n",
    "    N = (\n",
    "        len(X_tensor)\n",
    "        if cfg.ig_global_samples is None\n",
    "        else min(cfg.ig_global_samples, len(X_tensor))\n",
    "    )\n",
    "    idx = np.random.permutation(len(X_tensor))[:N]\n",
    "    X_sub = X_tensor[idx]\n",
    "\n",
    "    all_attrs = []\n",
    "    for i in range(0, N, cfg.ig_batch_size):\n",
    "        xb = X_sub[i : i + cfg.ig_batch_size].to(device).float()\n",
    "        attrs = compute_ig_batch(model, xb, ig_baseline_vec, n_steps=cfg.ig_steps)\n",
    "        all_attrs.append(attrs)\n",
    "\n",
    "    A = torch.cat(all_attrs, dim=0).abs().mean(dim=0).numpy()  # [F]\n",
    "    return A, idx  # mean |attribution| per feature\n",
    "\n",
    "\n",
    "def plot_topk_bar(\n",
    "    importances, feature_names, top_k=20, title=\"Global Feature Importance (IG)\"\n",
    "):\n",
    "    imp = np.asarray(importances)\n",
    "    k = min(top_k, len(imp))\n",
    "    order = np.argsort(imp)[::-1][:k]\n",
    "    labels = [feature_names[i] for i in order][::-1]\n",
    "    vals = imp[order][::-1]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.barh(labels, vals)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Mean |attribution|\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1064d314",
   "metadata": {},
   "source": [
    "Run IG global & local visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1415b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global (validation subset) ---\n",
    "global_imp_ig, used_idx = ig_global_importance(model_explain, X_val_tensor, EXPL_CFG)\n",
    "plot_topk_bar(\n",
    "    global_imp_ig,\n",
    "    FEATURE_NAMES,\n",
    "    top_k=EXPL_CFG.top_k,\n",
    "    title=\"Global Feature Importance (Integrated Gradients)\",\n",
    ")\n",
    "\n",
    "# --- Local (choose one sample from validation) ---\n",
    "SAMPLE_IDX = int(used_idx[0])  # pick first from subset; change as desired\n",
    "x_local = X_val_tensor[SAMPLE_IDX : SAMPLE_IDX + 1].to(device).float()\n",
    "attr_local = (\n",
    "    compute_ig_batch(model_explain, x_local, ig_baseline_vec, n_steps=EXPL_CFG.ig_steps)\n",
    "    .numpy()\n",
    "    .squeeze(0)\n",
    ")  # [F]\n",
    "\n",
    "# Plot top-K positive magnitude local contributions\n",
    "k = min(EXPL_CFG.top_k, len(attr_local))\n",
    "order = np.argsort(np.abs(attr_local))[::-1][:k]\n",
    "labels = [FEATURE_NAMES[i] for i in order][::-1]\n",
    "vals = attr_local[order][::-1]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(labels, vals)\n",
    "plt.title(f\"Local Attribution (IG) — val index {SAMPLE_IDX}\")\n",
    "plt.xlabel(\"Attribution (signed, logit space)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Inspect model logit/probability for the same sample\n",
    "with torch.no_grad():\n",
    "    logit = model_explain(x_local).item()\n",
    "    prob = torch.sigmoid(torch.tensor(logit)).item()\n",
    "print(f\"🔎 SAMPLE {SAMPLE_IDX} -> logit={logit:.4f}, prob={prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4820ee6",
   "metadata": {},
   "source": [
    "SHAP config & background/sample selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166d9bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapConfig:\n",
    "    background_size: int = 200  # background for DeepExplainer\n",
    "    sample_size: int = 1000  # validation samples to explain\n",
    "    top_k: int = 20\n",
    "\n",
    "\n",
    "SHAP_CFG = ShapConfig()\n",
    "\n",
    "# Select background from TRAIN\n",
    "bg_size = min(SHAP_CFG.background_size, len(X_train_tensor))\n",
    "bg_idx = np.random.permutation(len(X_train_tensor))[:bg_size]\n",
    "background = X_train_tensor[bg_idx].to(device).float()\n",
    "\n",
    "# Select a sample to explain from VAL\n",
    "sample_size = min(SHAP_CFG.sample_size, len(X_val_tensor))\n",
    "sample_idx = np.random.permutation(len(X_val_tensor))[:sample_size]\n",
    "X_sample = X_val_tensor[sample_idx].to(device).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0a5e1e",
   "metadata": {},
   "source": [
    "Run SHAP (DeepExplainer with Kernel fallback) + visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21e8462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe import of shap - if missing, skip SHAP cells with a clear message\n",
    "try:\n",
    "    import shap\n",
    "except Exception as e:\n",
    "    shap = None\n",
    "    print(\n",
    "        \"⚠️ shap not installed; SHAP visualizations will be skipped. Install with: pip install shap\"\n",
    "    )\n",
    "\n",
    "\n",
    "def shap_global_bar(\n",
    "    shap_vals, feature_names, top_k=20, title=\"Global Feature Importance (SHAP)\"\n",
    "):\n",
    "    # shap_vals: [N, F]\n",
    "    imp = np.mean(np.abs(shap_vals), axis=0)\n",
    "    k = min(top_k, len(imp))\n",
    "    order = np.argsort(imp)[::-1][:k]\n",
    "    labels = [feature_names[i] for i in order][::-1]\n",
    "    vals = imp[order][::-1]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.barh(labels, vals)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Mean |SHAP value|\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return imp\n",
    "\n",
    "\n",
    "# If shap is not available, skip the heavy computation and notify the user\n",
    "if shap is None:\n",
    "    print(\"Skipping SHAP computations because the 'shap' package is not available.\")\n",
    "else:\n",
    "    # Try DeepExplainer first (fast for PyTorch NNs), fallback to KernelExplainer if needed\n",
    "    try:\n",
    "        explainer = shap.DeepExplainer(model_explain, background)\n",
    "        shap_vals = explainer.shap_values(X_sample)  # returns list or array\n",
    "        if isinstance(shap_vals, list):\n",
    "            shap_vals = shap_vals[0]  # single-output model\n",
    "        shap_vals = np.array(shap_vals)  # [N, F]\n",
    "        print(\"✅ SHAP DeepExplainer succeeded.\")\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            \"⚠️ DeepExplainer failed, falling back to KernelExplainer. Reason:\", repr(e)\n",
    "        )\n",
    "        # KernelExplainer expects a function mapping numpy -> model outputs\n",
    "        model_explain.eval()\n",
    "\n",
    "        def f_np(x_np):\n",
    "            with torch.no_grad():\n",
    "                x_t = torch.from_numpy(x_np).to(device).float()\n",
    "                out = model_explain(x_t)  # logits\n",
    "                return out.detach().cpu().numpy()\n",
    "\n",
    "        # Use a smaller background for KernelExplainer for speed\n",
    "        bg_np = background[:50].detach().cpu().numpy()\n",
    "        kexpl = shap.KernelExplainer(f_np, bg_np)\n",
    "        shap_vals = kexpl.shap_values(\n",
    "            X_sample.detach().cpu().numpy(), nsamples=100\n",
    "        )  # tune nsamples for speed/accuracy\n",
    "        if isinstance(shap_vals, list):\n",
    "            shap_vals = shap_vals[0]\n",
    "        shap_vals = np.array(shap_vals)\n",
    "\n",
    "    # Global bar (top-K)\n",
    "    _ = shap_global_bar(\n",
    "        shap_vals,\n",
    "        FEATURE_NAMES,\n",
    "        top_k=SHAP_CFG.top_k,\n",
    "        title=\"Global Feature Importance (SHAP)\",\n",
    "    )\n",
    "\n",
    "    # Beeswarm summary (nice global picture)\n",
    "    try:\n",
    "        shap.summary_plot(\n",
    "            shap_vals,\n",
    "            X_sample.detach().cpu().numpy(),\n",
    "            feature_names=FEATURE_NAMES,\n",
    "            show=True,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Could not draw shap.summary_plot():\", repr(e))\n",
    "\n",
    "    # Dependence plot for top feature (relationship shape)\n",
    "    try:\n",
    "        top_feat_idx = int(np.argmax(np.mean(np.abs(shap_vals), axis=0)))\n",
    "        shap.dependence_plot(\n",
    "            top_feat_idx,\n",
    "            shap_vals,\n",
    "            X_sample.detach().cpu().numpy(),\n",
    "            feature_names=FEATURE_NAMES,\n",
    "            show=True,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Could not draw shap.dependence_plot():\", repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83a56e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small test: show binary prediction (0/1) and probability from the restored best model\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "if \"model_explain\" not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"model_explain not found. Run the checkpoint/load cell (Cell 23) first.\"\n",
    "    )\n",
    "\n",
    "model_explain.eval()\n",
    "\n",
    "# How many test samples to show (adjust as desired)\n",
    "N = 10\n",
    "X_all = X_test_tensor\n",
    "y_all = y_test_tensor\n",
    "N = min(N, len(X_all))\n",
    "\n",
    "xb = X_all[:N].to(device).float()\n",
    "with torch.no_grad():\n",
    "    logits = model_explain(xb)  # shape [N]\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "true = y_all[:N].cpu().numpy().astype(int)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"idx\": list(range(N)),\n",
    "        \"true\": true,\n",
    "        \"probability\": probs,\n",
    "        \"pred_binary\": preds,\n",
    "    }\n",
    ")\n",
    "print(df.to_string(index=False, float_format=\"{:.4f}\".format))\n",
    "\n",
    "acc = (preds == true).mean() if N > 0 else float(\"nan\")\n",
    "print(f\"\\nAccuracy on these {N} test samples: {acc:.3f}\")\n",
    "\n",
    "\n",
    "# Helper: predict a single sample tensor -> (pred_binary:int, probability:float)\n",
    "def predict_one(x_tensor):\n",
    "    x = x_tensor.to(device).float().unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        logit = model_explain(x).item()\n",
    "        prob = float(torch.sigmoid(torch.tensor(logit)).item())\n",
    "        pred = int(prob >= 0.5)\n",
    "    return pred, prob\n",
    "\n",
    "\n",
    "# Example: predict first test sample\n",
    "p, pr = predict_one(X_test_tensor[0])\n",
    "print(f\"\\nExample sample 0 -> pred={p}, prob={pr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d52816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random inputs for all FEATURES and predict — Streamlit-ready template\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "if \"FEATURE_NAMES\" not in globals():\n",
    "    raise RuntimeError(\"FEATURE_NAMES not found. Run startup cells first.\")\n",
    "if \"X_train_tensor\" not in globals():\n",
    "    raise RuntimeError(\"X_train_tensor not found. Run data-load cells first.\")\n",
    "if \"model_explain\" not in globals():\n",
    "    raise RuntimeError(\"model_explain not found. Load the checkpoint/model first.\")\n",
    "\n",
    "# Use training data statistics to synthesize realistic random inputs\n",
    "with torch.no_grad():\n",
    "    means = X_train_tensor.mean(dim=0).cpu().numpy()\n",
    "    stds = X_train_tensor.std(dim=0).cpu().numpy()\n",
    "    mins = X_train_tensor.min(dim=0).values.cpu().numpy()\n",
    "    maxs = X_train_tensor.max(dim=0).values.cpu().numpy()\n",
    "\n",
    "\n",
    "def sample_random_inputs(n_samples=1, clamp=True, scale=1.0, seed=None):\n",
    "    \"\"\"Generate n_samples random rows using (mean + scale*std*N(0,1)).\n",
    "    If clamp=True values are clipped to observed min/max to keep them plausible.\n",
    "    Returns a pandas.DataFrame and a torch.Tensor on CPU.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    z = np.random.randn(n_samples, len(FEATURE_NAMES))\n",
    "    X_rand = means[None, :] + (scale * stds)[None, :] * z\n",
    "    if clamp:\n",
    "        X_rand = np.minimum(np.maximum(X_rand, mins[None, :]), maxs[None, :])\n",
    "    df = pd.DataFrame(X_rand, columns=FEATURE_NAMES)\n",
    "    return df, torch.from_numpy(X_rand.astype(np.float32))\n",
    "\n",
    "\n",
    "# Example: create 3 random samples\n",
    "df_rand, X_rand_t = sample_random_inputs(n_samples=3, seed=42)\n",
    "print(\"Random input samples (first rows):\")\n",
    "print(df_rand.head().to_string(index=False))\n",
    "\n",
    "# Predict using model_explain\n",
    "model_explain.eval()\n",
    "with torch.no_grad():\n",
    "    xb = X_rand_t.to(device).float()\n",
    "    logits = model_explain(xb).cpu()\n",
    "    probs = torch.sigmoid(logits).numpy()\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "out_df = df_rand.copy()\n",
    "out_df[\"probability\"] = probs\n",
    "out_df[\"pred_binary\"] = preds\n",
    "\n",
    "print(\"Predictions for random inputs:\")\n",
    "print(out_df.to_string(index=False, float_format=\"{:.4f}\".format))\n",
    "\n",
    "\n",
    "# Helper to produce a dict of feature:value which is convenient for Streamlit inputs\n",
    "def random_input_dict(seed=None):\n",
    "    df, _ = sample_random_inputs(n_samples=1, seed=seed)\n",
    "    return df.iloc[0].to_dict()\n",
    "\n",
    "\n",
    "# Example streamlit-style dict for one sample\n",
    "example_dict = random_input_dict(seed=123)\n",
    "print(\"Example input dict (Streamlit form source):\")\n",
    "print(example_dict)\n",
    "\n",
    "\n",
    "# Single-sample prediction helper that accepts a feature-dict\n",
    "def predict_from_dict(feat_dict):\n",
    "    x = np.array([feat_dict[f] for f in FEATURE_NAMES], dtype=np.float32)[None, :]\n",
    "    xt = torch.from_numpy(x).to(device).float()\n",
    "    with torch.no_grad():\n",
    "        logit = model_explain(xt).item()\n",
    "        prob = float(torch.sigmoid(torch.tensor(logit)).item())\n",
    "        pred = int(prob >= 0.5)\n",
    "    return {\"pred_binary\": pred, \"probability\": prob}\n",
    "\n",
    "\n",
    "# Demo: predict from the example dict\n",
    "demo_out = predict_from_dict(example_dict)\n",
    "print(\"Demo prediction from example_dict:\", demo_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96331e5",
   "metadata": {},
   "source": [
    "\n",
    "# ✅ Embed Feature Metadata into Checkpoint\n",
    "\n",
    "This cell finds your latest `best_model.pt` (or a given path) and injects:\n",
    "- `feature_order` from `feature_order.json`\n",
    "- `scaler_params` from `scaler_params.json`\n",
    "\n",
    "so downstream apps can load the model and know **exactly** how inputs were prepared.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b5494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, glob, torch\n",
    "\n",
    "# Locate best_model.pt\n",
    "ckpt_path = None\n",
    "candidates = sorted(\n",
    "    glob.glob(\"**/best_model.pt\", recursive=True), key=os.path.getmtime, reverse=True\n",
    ")\n",
    "if candidates:\n",
    "    ckpt_path = candidates[0]\n",
    "else:\n",
    "    if os.path.exists(\"best_model.pt\"):\n",
    "        ckpt_path = \"best_model.pt\"\n",
    "\n",
    "assert (\n",
    "    ckpt_path is not None\n",
    "), \"Could not find best_model.pt. Please set ckpt_path manually.\"\n",
    "\n",
    "print(\"Patching checkpoint:\", ckpt_path)\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "# Load sidecar metadata\n",
    "feature_order = []\n",
    "scaler_params = {}\n",
    "if os.path.exists(\"feature_order.json\"):\n",
    "    feature_order = json.load(open(\"feature_order.json\", \"r\"))\n",
    "if os.path.exists(\"scaler_params.json\"):\n",
    "    scaler_params = json.load(open(\"scaler_params.json\", \"r\"))\n",
    "\n",
    "# Inject\n",
    "if isinstance(ckpt, dict):\n",
    "    ckpt[\"feature_order\"] = feature_order\n",
    "    ckpt[\"scaler_params\"] = scaler_params\n",
    "    torch.save(ckpt, ckpt_path)\n",
    "    print(\"✅ Injected feature_order & scaler_params into checkpoint.\")\n",
    "else:\n",
    "    print(\"⚠️ Checkpoint is not a dict; cannot inject metadata safely.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyVenv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
