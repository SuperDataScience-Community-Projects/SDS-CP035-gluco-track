{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "253b77db",
   "metadata": {},
   "source": [
    "\n",
    "# HW3 — CDC Diabetes Analysis (Model Development & Experimentation)\n",
    "\n",
    "This notebook scaffolds **Week 3 / Homework Set 3**:\n",
    "- Clean train/test split with **stratification**\n",
    "- **Preprocessing** (scaling numeric, one-hot for nominal as needed)\n",
    "- **SMOTE** on training only (to address class imbalance)\n",
    "- Baseline models (LogReg, DecisionTree, RandomForest, GradientBoosting, Naive Bayes, KNN)\n",
    "- Evaluation: Accuracy, Precision, Recall, F1, **ROC-AUC**, **PR-AUC**\n",
    "- **Confusion matrix**, **ROC** and **PR** curves\n",
    "- **Stratified K-Fold** cross-validation\n",
    "- **MLflow** experiment tracking (local) — optional\n",
    "- **Hyperparameter tuning** templates (RandomizedSearchCV)\n",
    "- Optional **threshold tuning** for recall targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4445c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_validate\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "                             confusion_matrix, roc_curve, precision_recall_curve, average_precision_score)\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Optional: MLflow (will no-op if not installed)\n",
    "try:\n",
    "    import mlflow\n",
    "    import mlflow.sklearn\n",
    "    MLFLOW_AVAILABLE = True\n",
    "except Exception:\n",
    "    MLFLOW_AVAILABLE = False\n",
    "    print(\"MLflow not available. To enable, install with: pip install mlflow\")\n",
    "    \n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2998bc1",
   "metadata": {},
   "source": [
    "\n",
    "## Load Data\n",
    "\n",
    "Choose one approach below and comment out the others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Option A: Load from local file you uploaded/mounted in Colab\n",
    "# Replace with your path or use the files UI to upload.\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # then: df = pd.read_csv('yourfile.csv')\n",
    "# df = pd.read_csv('your_local_file.csv')\n",
    "\n",
    "# Option B: Load from GitHub raw (if your CSV is public)\n",
    "# import pandas as pd\n",
    "# url = \"https://raw.githubusercontent.com/BartGoodell/SOME_REPO/SOME_BRANCH/path/to/file.csv\"\n",
    "# df = pd.read_csv(url)\n",
    "\n",
    "# Option C: Already-loaded DataFrame placeholder (replace this with real load)\n",
    "# For safety, we initialize an empty frame; replace with actual loading code.\n",
    "df = pd.DataFrame()  # TODO: replace with your actual dataset load\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "# display(df.head())  # uncomment after loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9594d31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8168cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "cdc_diabetes_health_indicators = fetch_ucirepo(id=891)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = cdc_diabetes_health_indicators.data.features\n",
    "y = cdc_diabetes_health_indicators.data.targets\n",
    "\n",
    "# metadata\n",
    "print(cdc_diabetes_health_indicators.metadata)\n",
    "\n",
    "# variable information\n",
    "print(cdc_diabetes_health_indicators.variables)\n",
    "\n",
    "df = pd.DataFrame()  # TODO: replace with your actual dataset load\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a883fcc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71a4ec7d",
   "metadata": {},
   "source": [
    "\n",
    "## Select Target and Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30494f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set your target column name here:\n",
    "TARGET = \"Diabetes_binary\"  # change if different\n",
    "\n",
    "assert TARGET in df.columns, f\"TARGET '{TARGET}' not found in columns: {df.columns.tolist()}\"\n",
    "\n",
    "X = df.drop(columns=[TARGET]).copy()\n",
    "y = df[TARGET].copy()\n",
    "\n",
    "print(\"Target distribution:\", y.value_counts(normalize=True).round(4).to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb02b5",
   "metadata": {},
   "source": [
    "\n",
    "## Optional Feature Engineering\n",
    "- `BMI_category` from `BMI` (CDC cutoffs)\n",
    "- `TotalHealthDays` = `PhysHlth` + `MentHlth`\n",
    "These are optional; the code guards for missing columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b0d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BMI_category (if BMI exists)\n",
    "if 'BMI' in X.columns:\n",
    "    def bmi_bucket(x):\n",
    "        if x < 18.5: return \"Underweight\"\n",
    "        if x < 25:   return \"Normal\"\n",
    "        if x < 30:   return \"Overweight\"\n",
    "        return \"Obese\"\n",
    "    X['BMI_category'] = X['BMI'].apply(bmi_bucket)\n",
    "\n",
    "# TotalHealthDays (if both exist)\n",
    "if set(['PhysHlth','MentHlth']).issubset(X.columns):\n",
    "    X['TotalHealthDays'] = X['PhysHlth'] + X['MentHlth']\n",
    "    \n",
    "print(\"Columns after feature engineering:\", X.columns.tolist()[:10], \"... (total:\", len(X.columns), \")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cc20c8",
   "metadata": {},
   "source": [
    "\n",
    "## Train / Test Split (Stratified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b9d400",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n",
    "print(\"Train class balance:\", y_train.value_counts(normalize=True).round(4).to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053802d5",
   "metadata": {},
   "source": [
    "\n",
    "## Preprocessing & Modeling Pipelines\n",
    "\n",
    "We detect numeric vs. non-numeric columns.  \n",
    "- Numeric: impute median, scale with StandardScaler.  \n",
    "- Categorical: impute most_frequent, one-hot encode.  \n",
    "**SMOTE** is applied **after** preprocessing and **before** the classifier (on training only) via `imblearn` pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d093636",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify numeric vs categorical columns from X_train\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = [c for c in X_train.columns if c not in numeric_features]\n",
    "\n",
    "numeric_tf = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "categorical_tf = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_tf, numeric_features),\n",
    "        (\"cat\", categorical_tf, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "def make_pipeline(clf):\n",
    "    # SMOTE applies only during fit on training data inside this pipeline\n",
    "    return ImbPipeline(steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
    "        (\"clf\", clf)\n",
    "    ])\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000, n_jobs=None, class_weight=None, random_state=RANDOM_STATE),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    \"NaiveBayes\": GaussianNB(),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=15)\n",
    "}\n",
    "pprint(models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710b0d32",
   "metadata": {},
   "source": [
    "\n",
    "## Fit Baselines & Evaluate on Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae24a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "curves = {}\n",
    "\n",
    "for name, clf in models.items():\n",
    "    pipe = make_pipeline(clf)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    \n",
    "    # predicted probabilities (required for AUCs)\n",
    "    if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\"):\n",
    "        y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        # fallback: decision_function if available\n",
    "        if hasattr(pipe.named_steps[\"clf\"], \"decision_function\"):\n",
    "            # scale decision function to 0-1 via min-max for AUC computation\n",
    "            df_scores = pipe.decision_function(X_test)\n",
    "            m, M = df_scores.min(), df_scores.max()\n",
    "            y_proba = (df_scores - m) / (M - m + 1e-12)\n",
    "        else:\n",
    "            y_proba = None\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    roc = roc_auc_score(y_test, y_proba) if y_proba is not None else np.nan\n",
    "    pr_auc = average_precision_score(y_test, y_proba) if y_proba is not None else np.nan\n",
    "    \n",
    "    results.append({\"model\": name, \"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1,\n",
    "                    \"roc_auc\": roc, \"pr_auc\": pr_auc})\n",
    "    \n",
    "    # store curves for plotting\n",
    "    if y_proba is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "        prec_c, rec_c, _ = precision_recall_curve(y_test, y_proba)\n",
    "        curves[name] = {\"fpr\": fpr, \"tpr\": tpr, \"prec\": prec_c, \"rec\": rec_c}\n",
    "        \n",
    "pd_results = pd.DataFrame(results).sort_values(by=[\"f1\", \"roc_auc\"], ascending=False).reset_index(drop=True)\n",
    "pd_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a25c81",
   "metadata": {},
   "source": [
    "\n",
    "## Confusion Matrix (Pick a Model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58ee42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_name = pd_results.iloc[0][\"model\"]\n",
    "print(\"Best by F1/ROC-AUC (heuristic):\", best_name)\n",
    "\n",
    "# Refit chosen model for CM plot\n",
    "pipe_best = make_pipeline(models[best_name])\n",
    "pipe_best.fit(X_train, y_train)\n",
    "y_pred_best = pipe_best.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(\"TN, FP, FN, TP:\", tn, fp, fn, tp)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title(f'Confusion Matrix - {best_name}')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, ['No', 'Yes'])\n",
    "plt.yticks(tick_marks, ['No', 'Yes'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96235e9",
   "metadata": {},
   "source": [
    "\n",
    "## ROC & Precision–Recall Curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ae2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot ROC\n",
    "plt.figure()\n",
    "for name, d in curves.items():\n",
    "    plt.plot(d[\"fpr\"], d[\"tpr\"], label=name)\n",
    "plt.plot([0,1],[0,1], linestyle='--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot PR\n",
    "plt.figure()\n",
    "for name, d in curves.items():\n",
    "    plt.plot(d[\"rec\"], d[\"prec\"], label=name)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curves\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ed14f",
   "metadata": {},
   "source": [
    "\n",
    "## Stratified K-Fold Cross-Validation (metrics on training data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36bcd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "cv_summary = []\n",
    "scoring = {\"accuracy\":\"accuracy\", \"precision\":\"precision\", \"recall\":\"recall\", \"f1\":\"f1\"}\n",
    "\n",
    "for name, clf in models.items():\n",
    "    pipe = make_pipeline(clf)\n",
    "    scores = cross_validate(pipe, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1, return_train_score=False)\n",
    "    row = {\"model\": name}\n",
    "    for k, v in scoring.items():\n",
    "        row[f\"cv_{k}_mean\"] = scores[f\"test_{k}\"].mean()\n",
    "        row[f\"cv_{k}_std\"] = scores[f\"test_{k}\"].std()\n",
    "    cv_summary.append(row)\n",
    "\n",
    "pd_cv = pd.DataFrame(cv_summary).sort_values(by=[\"cv_f1_mean\",\"cv_recall_mean\"], ascending=False)\n",
    "pd_cv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ec7a65",
   "metadata": {},
   "source": [
    "\n",
    "## (Optional) MLflow Logging\n",
    "Run this cell to log models/metrics to a local MLflow experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa43e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if MLFLOW_AVAILABLE:\n",
    "    mlflow.set_experiment(\"HW3_CDC_Diabetes\")\n",
    "    for name, clf in models.items():\n",
    "        with mlflow.start_run(run_name=name):\n",
    "            pipe = make_pipeline(clf)\n",
    "            pipe.fit(X_train, y_train)\n",
    "            y_pred = pipe.predict(X_test)\n",
    "            if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\"):\n",
    "                y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "            else:\n",
    "                y_proba = None\n",
    "            mlflow.log_metric(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "            mlflow.log_metric(\"precision\", precision_score(y_test, y_pred, zero_division=0))\n",
    "            mlflow.log_metric(\"recall\", recall_score(y_test, y_pred, zero_division=0))\n",
    "            mlflow.log_metric(\"f1\", f1_score(y_test, y_pred, zero_division=0))\n",
    "            if y_proba is not None:\n",
    "                mlflow.log_metric(\"roc_auc\", roc_auc_score(y_test, y_proba))\n",
    "                mlflow.log_metric(\"pr_auc\", average_precision_score(y_test, y_proba))\n",
    "            # Log basic params if available\n",
    "            try:\n",
    "                mlflow.log_params(pipe.named_steps[\"clf\"].get_params())\n",
    "            except Exception:\n",
    "                pass\n",
    "            mlflow.sklearn.log_model(pipe, artifact_path=\"model\")\n",
    "    print(\"Logged runs. In Colab, you can view the MLflow UI locally if you forward ports, or just rely on the runs list.\")\n",
    "else:\n",
    "    print(\"MLflow not installed; skipping logging.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74d5f9",
   "metadata": {},
   "source": [
    "\n",
    "## Hyperparameter Tuning (Templates)\n",
    "Edit param grids as needed. RandomizedSearchCV shown for speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d85371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Example: Gradient Boosting\n",
    "gb_pipe = make_pipeline(GradientBoostingClassifier(random_state=RANDOM_STATE))\n",
    "gb_params = {\n",
    "    \"clf__n_estimators\": [100, 200, 300],\n",
    "    \"clf__learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"clf__max_depth\": [2, 3, 4],\n",
    "    \"clf__subsample\": [0.7, 0.85, 1.0]\n",
    "}\n",
    "gb_search = RandomizedSearchCV(gb_pipe, gb_params, n_iter=12, scoring=\"f1\", cv=3, n_jobs=-1, random_state=RANDOM_STATE)\n",
    "# gb_search.fit(X_train, y_train)\n",
    "# print(\"Best GB params:\", gb_search.best_params_)\n",
    "# print(\"Best GB F1:\", gb_search.best_score_)\n",
    "\n",
    "# Example: Logistic Regression\n",
    "lr_pipe = make_pipeline(LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))\n",
    "lr_params = {\n",
    "    \"clf__C\": np.logspace(-3, 2, 10),\n",
    "    \"clf__penalty\": [\"l2\"],\n",
    "    \"clf__class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "lr_search = RandomizedSearchCV(lr_pipe, lr_params, n_iter=10, scoring=\"f1\", cv=3, n_jobs=-1, random_state=RANDOM_STATE)\n",
    "\n",
    "# Example: Random Forest\n",
    "rf_pipe = make_pipeline(RandomForestClassifier(random_state=RANDOM_STATE))\n",
    "rf_params = {\n",
    "    \"clf__n_estimators\": [200, 400, 600],\n",
    "    \"clf__max_depth\": [None, 6, 10, 14],\n",
    "    \"clf__max_features\": [\"sqrt\", \"log2\", 0.5, 0.8],\n",
    "    \"clf__min_samples_split\": [2, 5, 10],\n",
    "    \"clf__min_samples_leaf\": [1, 2, 4]\n",
    "}\n",
    "rf_search = RandomizedSearchCV(rf_pipe, rf_params, n_iter=12, scoring=\"f1\", cv=3, n_jobs=-1, random_state=RANDOM_STATE)\n",
    "\n",
    "print(\"Search objects prepared. Uncomment .fit(...) lines to run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b2dcff",
   "metadata": {},
   "source": [
    "\n",
    "## Threshold Tuning (Recall Target)\n",
    "Find a probability threshold that achieves a target recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba75c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_recall = 0.85  # adjust as needed\n",
    "\n",
    "pipe_lr = make_pipeline(LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "\n",
    "y_proba_lr = pipe_lr.predict_proba(X_test)[:, 1]\n",
    "prec, rec, thr = precision_recall_curve(y_test, y_proba_lr)\n",
    "\n",
    "# Find threshold meeting or exceeding target_recall\n",
    "idx = np.where(rec >= target_recall)[0]\n",
    "if len(idx):\n",
    "    best_idx = idx[-1]  # highest threshold that still meets recall\n",
    "    chosen_thr = thr[best_idx-1] if best_idx > 0 else 0.5\n",
    "else:\n",
    "    chosen_thr = 0.5  # fallback\n",
    "\n",
    "y_pred_thr = (y_proba_lr >= chosen_thr).astype(int)\n",
    "\n",
    "print(f\"Chosen threshold: {chosen_thr:.3f}\")\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_thr, zero_division=0))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_thr, zero_division=0))\n",
    "print(\"F1:\", f1_score(y_test, y_pred_thr, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970f4c6d",
   "metadata": {},
   "source": [
    "\n",
    "## Save Results (CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf960f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd_results.to_csv(\"hw3_model_results.csv\", index=False)\n",
    "print(\"Saved hw3_model_results.csv in current working directory.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
